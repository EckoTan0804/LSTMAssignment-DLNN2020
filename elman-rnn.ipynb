{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "import sys\n",
    "\n",
    "\n",
    "# Since numpy doesn't have a function for sigmoid\n",
    "# We implement it manually here\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# The derivative of the sigmoid function\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "# The derivative of the tanh function\n",
    "def dtanh(x):\n",
    "    return 1 - x * x\n",
    "\n",
    "\n",
    "# The numerically stable softmax implementation\n",
    "def softmax(x):\n",
    "    # assuming x shape is [feature_size, batch_size]\n",
    "    e_x = np.exp(x - np.max(x, axis=0))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/input.short.txt', 'r').read()\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "data_size, vocab_size = len(data), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'C',\n",
       " 'F',\n",
       " 'I',\n",
       " 'L',\n",
       " 'M',\n",
       " 'S',\n",
       " 'W',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters deciding the network size\n",
    "emb_size = 4  # word/character embedding size\n",
    "seq_length = 32  # number of steps to unroll the RNN for the truncated back-propagation algorithm\n",
    "hidden_size = 32\n",
    "# learning rate for the Adagrad algorithm. (this one is not 'optimized', only required to make the model learn)\n",
    "learning_rate = 0.02\n",
    "std = 0.02  # The standard deviation for parameter initilization\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "# Here we initialize the parameters based an random uniform distribution, with the std of 0.01\n",
    "\n",
    "# word embedding: each character in the vocabulary is mapped to a vector with $emb_size$ neurons\n",
    "# Transform one-hot vectors to embedding X\n",
    "Wex = np.random.randn(emb_size, vocab_size) * std\n",
    "\n",
    "# weight to transform input X to hidden H\n",
    "Wxh = np.random.randn(hidden_size, emb_size) * std\n",
    "\n",
    "# weight to transform previous hidden states H_{t-1} to hidden H_t\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * std  # hidden to hidden\n",
    "\n",
    "# Output layer: transforming the hidden states H to output layer\n",
    "Why = np.random.randn(vocab_size, hidden_size) * std  # hidden to output\n",
    "\n",
    "# The biases are typically initialized as zeros. But sometimes people init them with uniform distribution too.\n",
    "bh = np.random.randn(hidden_size, 1) * std  # hidden bias\n",
    "by = np.random.randn(vocab_size, 1) * std  # hidden bias\n",
    "\n",
    "# These variables are momentums for the Adagrad algorithm\n",
    "# Each parameter in the network needs one momentum correspondingly\n",
    "mWex, mWxh, mWhh, mWhy = np.zeros_like(Wex), np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606,)\n"
     ]
    }
   ],
   "source": [
    "# this will load the data into memory\n",
    "data_stream = np.asarray([char_to_ix[char] for char in data])\n",
    "print(data_stream.shape)\n",
    "\n",
    "bound = (data_stream.shape[0] // (seq_length * batch_size)) * (seq_length * batch_size)\n",
    "cut_stream = data_stream[:bound]\n",
    "cut_stream = np.reshape(cut_stream, (batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  8, 22, 30, 31, 32,  1,  7, 22, 32, 22, 37, 18, 27,  4,  0,\n",
       "       13, 18,  1, 14, 30, 18,  1, 14, 16, 16, 28, 33, 27, 32, 18, 17,  1,\n",
       "       29, 28, 28, 30,  1, 16, 22, 32, 22, 37, 18, 27, 31,  2,  1, 32, 21,\n",
       "       18,  1, 29, 14, 32, 30, 22, 16, 22, 14, 27, 31,  1, 20, 28, 28, 17,\n",
       "        3,  0, 13, 21, 14, 32,  1, 14, 33, 32, 21, 28, 30, 22, 32, 36,  1,\n",
       "       31, 33, 30, 19, 18, 22, 32, 31,  1, 28, 27,  1, 35, 28, 33, 25, 17,\n",
       "        1, 30, 18, 25, 22, 18, 34, 18,  1, 33, 31,  4,  1, 22, 19,  1, 32,\n",
       "       21, 18, 36,  0, 35, 28, 33, 25, 17,  1, 36, 22, 18, 25, 17,  1, 33,\n",
       "       31,  1, 15, 33, 32,  1, 32, 21, 18,  1, 31, 33, 29, 18, 30, 19, 25,\n",
       "       33, 22, 32, 36,  2,  1, 35, 21, 22, 25, 18,  1, 22, 32,  1, 35, 18,\n",
       "       30, 18,  0, 35, 21, 28, 25, 18, 31, 28, 26, 18,  2,  1, 35, 18,  1,\n",
       "       26, 22, 20, 21, 32,  1, 20, 33, 18, 31, 31,  1, 32, 21, 18, 36,  1,\n",
       "       30, 18, 25, 22, 18, 34, 18, 17,  1, 33, 31,  1, 21, 33, 26, 14, 27,\n",
       "       18, 25, 36,  5,  0, 15, 33, 32,  1, 32, 21, 18, 36,  1, 32, 21, 22,\n",
       "       27, 24,  1, 35, 18,  1, 14, 30, 18,  1, 32, 28, 28,  1, 17, 18, 14,\n",
       "       30,  4,  1, 32, 21, 18,  1, 25, 18, 14, 27, 27, 18, 31, 31,  1, 32,\n",
       "       21, 14, 32,  0, 14, 19, 19, 25, 22, 16, 32, 31,  1, 33, 31,  2,  1,\n",
       "       32, 21, 18,  1, 28, 15, 23, 18, 16, 32,  1, 28, 19,  1, 28, 33, 30,\n",
       "        1, 26, 22, 31, 18, 30, 36,  2,  1, 22, 31,  1, 14, 31,  1, 14, 27,\n",
       "        0, 22, 27, 34, 18, 27, 32, 28, 30, 36,  1, 32, 28,  1, 29, 14, 30,\n",
       "       32, 22, 16, 33, 25, 14, 30, 22, 31, 18,  1, 32, 21, 18, 22, 30,  1,\n",
       "       14, 15, 33, 27, 17, 14, 27, 16, 18,  5,  1, 28, 33, 30,  0, 31, 33,\n",
       "       19, 19, 18, 30, 14, 27, 16, 18,  1, 22, 31,  1, 14,  1, 20, 14, 22,\n",
       "       27,  1, 32, 28,  1, 32, 21, 18, 26,  1, 10, 18, 32,  1, 33, 31,  1,\n",
       "       30, 18, 34, 18, 27, 20, 18,  1, 32, 21, 22, 31,  1, 35, 22, 32, 21,\n",
       "        0, 28, 33, 30,  1, 29, 22, 24, 18, 31,  2,  1, 18, 30, 18,  1, 35,\n",
       "       18,  1, 15, 18, 16, 28, 26, 18,  1, 30, 14, 24, 18, 31,  4,  1, 19,\n",
       "       28, 30,  1, 32, 21, 18,  1, 20, 28, 17, 31,  1, 24, 27, 28, 35,  1,\n",
       "        9,  0, 31, 29, 18, 14, 24,  1, 32, 21, 22, 31,  1, 22, 27,  1, 21,\n",
       "       33, 27, 20, 18, 30,  1, 19, 28, 30,  1, 15, 30, 18, 14, 17,  2,  1,\n",
       "       27, 28, 32,  1, 22, 27,  1, 32, 21, 22, 30, 31, 32,  1, 19, 28, 30,\n",
       "        1, 30, 18, 34, 18, 27, 20, 18,  3,  0,  0, 12, 18, 16, 28, 27, 17,\n",
       "        1,  7, 22, 32, 22, 37, 18, 27,  4,  0, 13, 28, 33, 25, 17,  1, 36,\n",
       "       28, 33,  1, 29, 30, 28, 16, 18, 18, 17,  1, 18, 31, 29, 18, 16, 22,\n",
       "       14, 25, 25, 36,  1, 14, 20, 14, 22, 27, 31, 32,  1,  7, 14, 22, 33,\n",
       "       31,  1, 11, 14, 30, 16, 22, 33, 31,  6,  0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, labels, memory, batch_size=1):\n",
    "    prev_h = memory\n",
    "    \"\"\"\n",
    "    # here we use dictionaries to store the activations over time\n",
    "    # note from back-propagation implementation:\n",
    "    # back-propagation uses dynamic programming to estimate gradients efficiently\n",
    "    # so we need to store the activations over the course of the forward pass\n",
    "    # in the backward pass we will use the activations to compute the gradients\n",
    "    # (otherwise we will need to recompute them)\n",
    "    \"\"\"\n",
    "\n",
    "    # those variables stand for:\n",
    "    # xs: inputs to the RNNs at timesteps (embeddings)\n",
    "    # cs: characters at timesteps\n",
    "    # hs: hidden states at timesteps\n",
    "    # ys: output layers at timesteps\n",
    "    # ps: probability distributions at timesteps\n",
    "    xs, cs, hs, os, ps, ys = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    # the first memory (before training) is the previous (or initial) hidden state\n",
    "    hs[-1] = np.copy(prev_h)\n",
    "\n",
    "    # the loss will be accumulated over time\n",
    "    loss = 0\n",
    "    \n",
    "    print(\"inputs:\", inputs)\n",
    "\n",
    "    for t in range(inputs.shape[1]):\n",
    "\n",
    "        # one-hot vector representation for character input at time t\n",
    "        cs[t] = np.zeros((vocab_size, batch_size))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            cs[t][inputs[b][t]][b] = 1\n",
    "            \n",
    "#         print(\"cs:\", cs)\n",
    "        \n",
    "\n",
    "        # transform the one hot vector to embedding\n",
    "        # x = Wemb x c\n",
    "        xs[t] = np.dot(Wex, cs[t])\n",
    "\n",
    "        # computation for the hidden state of the network\n",
    "        # H = tanh ( Wh . H + Wx . x )\n",
    "        h_pre_activation = np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh\n",
    "        hs[t] = np.tanh(h_pre_activation)\n",
    "\n",
    "        # output layer:\n",
    "        # this is the unnormalized log probabilities for next chars (across all chars in the vocabulary)\n",
    "        os[t] = np.dot(Why, hs[t]) + by\n",
    "\n",
    "        # softmax layer to get normalized probabilities:\n",
    "        ps[t] = softmax(os[t])\n",
    "\n",
    "        # the label is also an one-hot vector\n",
    "        ys[t] = np.zeros((vocab_size, batch_size))\n",
    "        for b in range(batch_size):\n",
    "            ys[t][labels[b][t]][b] = 1\n",
    "            \n",
    "#         print(\"ys:\", ys)\n",
    "\n",
    "        # cross entropy loss at time t:\n",
    "        loss_t = np.sum(-np.log(ps[t]) * ys[t])\n",
    "\n",
    "        loss += loss_t\n",
    "\n",
    "    # packaging the activations to use in the backward pass\n",
    "    activations = (xs, cs, hs, os, ps, ys)\n",
    "    last_hidden = hs[inputs.shape[1] - 1]\n",
    "    return loss, activations, last_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(activations, clipping=True, scale=True):\n",
    "    \"\"\"\n",
    "    during the backward pass we follow the track of the forward pass\n",
    "    the activations are needed so that we can avoid unnecessary re-computation\n",
    "    \"\"\"\n",
    "\n",
    "    # Gradient initialization\n",
    "    # Each parameter has a corresponding gradient (of the loss with respect to that gradient)\n",
    "    dWex, dWxh, dWhh, dWhy = np.zeros_like(Wex), np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "    xs, cs, hs, os, ps, ys = activations\n",
    "\n",
    "    # here we need the gradient w.r.t to the hidden layer at the final time step\n",
    "    # since this hidden layer is not connected to any future (final time step)\n",
    "    # then we can initialize it as zero vectors\n",
    "    dh = np.zeros_like(hs[0])\n",
    "    bsz = dh.shape[-1]\n",
    "\n",
    "    # the backward pass starts from the final step of the chain in the forward pass\n",
    "    for t in reversed(range(inputs.shape[1])):\n",
    "\n",
    "        # first, we need to compute the gradients of the variable closest to the loss function,\n",
    "        # which is the softmax output p\n",
    "        # but here I skip it directly to the gradients of the unnormalized scores o because\n",
    "        # basically dL / do = p - y\n",
    "        # from the cross entropy gradients. (the explanation is a bit too long to write here)\n",
    "        do = ps[t] - ys[t]\n",
    "\n",
    "        if scale:\n",
    "            do = do / bsz\n",
    "\n",
    "        # the gradients w.r.t to the weights and the bias that were used to create o[t]\n",
    "        dWhy += np.dot(do, hs[t].T)\n",
    "        dby += np.sum(do, axis=-1, keepdims=True)\n",
    "\n",
    "        # because h is connected to both o and the next h, we sum the gradients up\n",
    "        dh = np.dot(Why.T, do) + dh\n",
    "\n",
    "        # backprop through the activation function (tanh)\n",
    "        dtanh_h = 1 - hs[t] * hs[t]\n",
    "        dh_pre_activation = dtanh_h * dh  # because h = tanh(h_pre_activation)\n",
    "\n",
    "        # next, since  H = tanh ( Wh . H + Wx . x + bh )\n",
    "        # we use dh to backprop to dWh and dWx\n",
    "\n",
    "        # gradient of the bias and weight, this is similar to dby and dWhy\n",
    "        # for the H term\n",
    "        dbh += np.sum(dh_pre_activation, axis=-1, keepdims=True)\n",
    "        dWhh += np.dot(dh_pre_activation, hs[t - 1].T)\n",
    "        # we need this term for the recurrent connection (previous bptt step needs this)\n",
    "        dh = np.dot(Whh.T, dh_pre_activation)\n",
    "\n",
    "        # similarly for the x term\n",
    "        dWxh += np.dot(dh_pre_activation, xs[t].T)\n",
    "\n",
    "        # backward through the embedding\n",
    "        dx = np.dot(Wxh.T, dh_pre_activation)\n",
    "\n",
    "        # finally backward to the embedding projection\n",
    "        dWex += np.dot(dx, cs[t].T)\n",
    "\n",
    "    if clipping:\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)  # clip to mitigate exploding gradients\n",
    "\n",
    "    gradients = (dWex, dWxh, dWhh, dWhy, dbh, dby)\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    c = np.zeros((vocab_size, 1))\n",
    "    c[seed_ix] = 1\n",
    "    generated_chars = []\n",
    "    for t in range(n):\n",
    "        x = np.dot(Wex, c)\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        o = np.dot(Why, h) + by\n",
    "        p = softmax(o)\n",
    "\n",
    "        # the the distribution, we randomly generate samples:\n",
    "        ix = np.random.multinomial(1, p.ravel())\n",
    "        c = np.zeros((vocab_size, 1))\n",
    "\n",
    "        for j in range(len(ix)):\n",
    "            if ix[j] == 1:\n",
    "                index = j\n",
    "        c[index] = 1\n",
    "        generated_chars.append(index)\n",
    "\n",
    "    return generated_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "data_length = cut_stream.shape[1]\n",
    "\n",
    "# I am not perfectly sure about this (learnt from others that the initial \"perplexity\" of the model\n",
    "# should be the vocabulary for every position. So this is the loss at iteration 0\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "while n < 5:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p + seq_length + 1 >= data_length or n == 0:\n",
    "        hprev = np.zeros((hidden_size, batch_size))  # reset RNN memory\n",
    "        p = 0  # go back to start of data\n",
    "\n",
    "    inputs = cut_stream[:, p:p + seq_length]\n",
    "    targets = cut_stream[:, p + 1:p + 1 + seq_length]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        h_zero = np.zeros((hidden_size, 1))\n",
    "        sample_ix = sample(h_zero, inputs[0][0], 1500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, activations, hprev = forward(inputs, targets, hprev, batch_size=batch_size)\n",
    "    gradients = backward(activations)\n",
    "    dWex, dWxh, dWhh, dWhy, dbh, dby = gradients\n",
    "    smooth_loss = smooth_loss * 0.999 + loss / batch_size * 0.001\n",
    "    if n % 20 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wex, Wxh, Whh, Why, bh, by],\n",
    "                                  [dWex, dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWex, mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
