{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1 - x * x\n",
    "\n",
    "\n",
    "# The numerically stable softmax implementation\n",
    "def softmax(x):\n",
    "    # assuming x shape is [feature_size, batch_size]\n",
    "    e_x = np.exp(x - np.max(x, axis=0))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open(\"data/input.txt\", \"r\").read()  # should be simple plain text file\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "std = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 16\n",
    "hidden_size = 256  # size of hidden layer of neurons\n",
    "seq_length = 128  # number of steps to unroll the RNN for\n",
    "learning_rate = 5e-2\n",
    "max_updates = 500000\n",
    "batch_size = 32\n",
    "\n",
    "concat_size = emb_size + hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_check = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "# char embedding parameters\n",
    "Wex = np.random.randn(emb_size, vocab_size) * std  # embedding layer\n",
    "\n",
    "# LSTM parameters\n",
    "Wf = np.random.randn(hidden_size, concat_size) * std  # forget gate\n",
    "Wi = np.random.randn(hidden_size, concat_size) * std  # input gate\n",
    "Wo = np.random.randn(hidden_size, concat_size) * std  # output gate\n",
    "Wc = np.random.randn(hidden_size, concat_size) * std  # c term\n",
    "\n",
    "bf = np.zeros((hidden_size, 1))  # forget bias\n",
    "bi = np.zeros((hidden_size, 1))  # input bias\n",
    "bo = np.zeros((hidden_size, 1))  # output bias\n",
    "bc = np.zeros((hidden_size, 1))  # memory bias\n",
    "\n",
    "# Output layer parameters\n",
    "Why = np.random.randn(vocab_size, hidden_size) * std  # hidden to output\n",
    "by = np.random.randn(vocab_size, 1) * std  # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,)\n"
     ]
    }
   ],
   "source": [
    "data_stream = np.asarray([char_to_ix[char] for char in data])\n",
    "print(data_stream.shape)\n",
    "\n",
    "bound = (data_stream.shape[0] // (seq_length * batch_size)) * (seq_length * batch_size)\n",
    "cut_stream = data_stream[:bound]\n",
    "cut_stream = np.reshape(cut_stream, (batch_size, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, targets, memory):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    hprev, cprev = memory\n",
    "    \n",
    "    xs = {} # input\n",
    "    wes = {} # word embeddings\n",
    "    hs = {} # hidden states\n",
    "    zs = {} # concat of last hidden state and input\n",
    "    cs = {} # cell states\n",
    "    fs = {} # forget gates\n",
    "    ins = {} # input gates\n",
    "    c_s = {} # candidate cell states\n",
    "    os = {} # output gates\n",
    "    ys = {} # unnormalized output (before softmax)\n",
    "    ps = {} # normalized probability output (after softmax)\n",
    "    ls = {} # labels (ground truth)\n",
    "    \n",
    "    hs[-1] = np.copy(hprev)\n",
    "    cs[-1] = np.copy(cprev)\n",
    "\n",
    "    loss = 0\n",
    "    input_length = inputs.shape[0]\n",
    "\n",
    "    # forward pass\n",
    "    for t in range(input_length):\n",
    "        xs[t] = np.zeros((vocab_size, batch_size))  # encode in 1-of-k representation\n",
    "        for b in range(batch_size):\n",
    "            xs[t][inputs[t][b]][b] = 1\n",
    "\n",
    "        # convert word indices to word embeddings\n",
    "#         wes[t] = np.dot(Wex, xs[t])\n",
    "        wes[t] = Wex @ xs[t]\n",
    "\n",
    "        # LSTM cell operation\n",
    "        # first concatenate the input and h to get z\n",
    "        # Note: Use np.vstack instead of np.row_stack\n",
    "        zs[t] = np.vstack((hs[t - 1], wes[t]))\n",
    "        \n",
    "\n",
    "        # compute the forget gate\n",
    "        # f = sigmoid(Wf * z + bf)\n",
    "        fs[t] = sigmoid(Wf @ zs[t] + bf)\n",
    "        \n",
    "\n",
    "        # compute the input gate\n",
    "        # i = sigmoid(Wi * z + bi)\n",
    "        ins[t] = sigmoid(Wi @ zs[t] + bi)\n",
    "        \n",
    "        # compute the candidate memory\n",
    "        # c_ = tanh(Wc * z + bc)\n",
    "        c_s[t] = np.tanh(Wc @ zs[t] + bc)\n",
    "        \n",
    "\n",
    "        # new memory: applying forget gate on the previous memory\n",
    "        # and then adding the input gate on the candidate memory\n",
    "        cs[t] = fs[t] * cs[t-1] + ins[t] * c_s[t]\n",
    "        \n",
    "\n",
    "        # output gate\n",
    "        # o = sigmoid(Wo * z + bo)\n",
    "        os[t] = sigmoid(Wo @ zs[t] + bo)\n",
    "        \n",
    "\n",
    "        # hidden state\n",
    "        hs[t] = os[t] * np.tanh(cs[t])\n",
    "        \n",
    "        \n",
    "        # output layer (unnormalized)\n",
    "        ys[t] = Why @ hs[t] + by\n",
    "        \n",
    "        \n",
    "        # Softmax layer to get normalized probabilities\n",
    "        ps[t] = softmax(ys[t])\n",
    "        \n",
    "\n",
    "\n",
    "        # label\n",
    "        ls[t] = np.zeros((vocab_size, batch_size))\n",
    "        for b in range(batch_size):\n",
    "            ls[t][targets[t][b]][b] = 1\n",
    "\n",
    "        # cross-entropy loss\n",
    "        loss_t = np.sum(-np.log(ps[t]) * ls[t])\n",
    "        loss += loss_t\n",
    "        # loss += -np.log(ps[t][targets[t],0])\n",
    "\n",
    "#         if dim_check:\n",
    "#             print(\"z:\", zs[t].shape)\n",
    "#             print(\"f:\", fs[t].shape)\n",
    "#             print(\"i:\", ins[t].shape)\n",
    "#             print(\"c_:\", c_s[t].shape)\n",
    "#             print(\"c:\", cs[t].shape)\n",
    "#             print(\"o:\", os[t].shape)\n",
    "#             print(\"h:\", hs[t].shape)\n",
    "#             print(\"ys[t]:\", ys[t].shape)\n",
    "#             print(\"p:\", ps[t].shape)\n",
    "#             print(\"l:\", ls[t].shape)\n",
    "#             print(\"loss:\", loss_t.shape)\n",
    "#             print(\"-------------\")\n",
    "        \n",
    "    activations = (xs, wes, hs, ys, ps, cs, zs, ins, c_s, ls, os, fs)\n",
    "    memory = (hs[input_length - 1], cs[input_length - 1])\n",
    "\n",
    "    return loss, activations, memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(activations, clipping=True):\n",
    "    xs, wes, hs, ys, ps, cs, zs, ins, c_s, ls, os, fs = activations\n",
    "\n",
    "    # backward pass: compute gradients going backwards\n",
    "    # Here we allocate memory for the gradients\n",
    "    dWex, dWhy = np.zeros_like(Wex), np.zeros_like(Why)\n",
    "    dby = np.zeros_like(by)\n",
    "    dWf, dWi, dWc, dWo = np.zeros_like(Wf), np.zeros_like(Wi), np.zeros_like(Wc), np.zeros_like(Wo)\n",
    "    dbf, dbi, dbc, dbo = np.zeros_like(bf), np.zeros_like(bi), np.zeros_like(bc), np.zeros_like(bo)\n",
    "\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dcnext = np.zeros_like(cs[0])\n",
    "\n",
    "    input_length = len(xs)\n",
    "\n",
    "    # back propagation through time starts here\n",
    "    for t in reversed(range(input_length)):\n",
    "        \n",
    "        # Softmax loss gradient\n",
    "        dy = ls[t] - ys[t]\n",
    "        \n",
    "        \n",
    "        # Note:\n",
    "        # About matrix transpose in gradient, check out\n",
    "        # https://cs231n.github.io/optimization-2/#staged\n",
    "        \n",
    "        # Hidden to output gradient\n",
    "        # Gradient for ys[t] = Why @ hs[t] + by\n",
    "        dWhy = dy @ (hs[t].T)\n",
    "        dby = dy[:, -1]\n",
    "        dh = (Why.T) @ dy + dhnext # need to add gradients from the next hidden state\n",
    "        \n",
    "        \n",
    "        # Gradient for hs[t] = os[t] * np.tanh(cs[t])\n",
    "        ## Gradient for os[t] \n",
    "        do = np.tanh(cs[t]) * dh\n",
    "#         do = dsigmoid(os[t]) * do\n",
    "        ## Gradient for cs[t]\n",
    "        dc = os[t] * dh * dtanh(cs[t])\n",
    "        dc = dc + dcnext\n",
    "        \n",
    "        \n",
    "        # Gradient for cs[t] = fs[t] * cs[t-1] + ins[t] * c_s[t]\n",
    "        ## Gradient for fs[t]\n",
    "        df = cs[t - 1] * dc\n",
    "#         df = dsigmoid(fs[t]) * df\n",
    "        ## Gradient for ins[t]\n",
    "        di = c_s[t] * dc\n",
    "#         di = dsigmoid(ins[t]) * di\n",
    "        ## Gradient for c_s[t]\n",
    "        dc_ = ins[t] * dc\n",
    "#         dc_ = dtanh(c_s[t]) * dc_\n",
    "        \n",
    "    \n",
    "        # Gate gradients\n",
    "        ## Forget gate\n",
    "        ## Gradients for fs[t] = sigmoid(Wf @ zs[t] + bf)\n",
    "        df = dsigmoid(fs[t]) * df\n",
    "        dWf = df @ (zs[t].T)\n",
    "        dbf = df\n",
    "        dzf = Wf.T @ df\n",
    "\n",
    "        ## Input gate\n",
    "        ## Gradients for ins[t] = sigmoid(Wi @ zs[t] + bi)\n",
    "        di = dsigmoid(ins[t]) * di\n",
    "        dWi = di @ (zs[t].T)\n",
    "        dbi = di\n",
    "        dzi = Wi.T @ di \n",
    "\n",
    "        ## Output gate\n",
    "        ## Gradients for s[t] = sigmoid(Wo @ zs[t] + bo)\n",
    "        do = dsigmoid(os[t]) * do\n",
    "        dWo = do @ (zs[t].T)\n",
    "        dbo = do\n",
    "        dzo = Wo.T @ do\n",
    "        \n",
    "        ## Candidate cell state\n",
    "        ## Gradients for c_s[t] = np.tanh(Wc @ zs[t] + bc)\n",
    "        dc_ = dtanh(c_s[t]) * dc_\n",
    "        dWc = dc_ @ (zs[t].T)\n",
    "        dbc = dc_\n",
    "        dzc = Wc.T @ dc_\n",
    "        \n",
    "        # zs[t] (concat of hs[t-1] and wes[t]) is used in multiple gates, \n",
    "        # thus the gradient must be accumulated\n",
    "        dz = dzo + dzc + dzi + dzf\n",
    "        \n",
    "        # Gradient for hs[t-1]\n",
    "        dhnext = dz[:hidden_size, :]\n",
    "        \n",
    "        # Gradient for cs[t-1] in cs[t] = fs[t] * cs[t-1] + ins[t] * c_s[t]\n",
    "        dcnext = fs[t] * dc\n",
    "        \n",
    "        if dim_check:\n",
    "            print(\"dy:\", dy.shape)\n",
    "            \n",
    "            print(\"dWhy:\", dWhy.shape)\n",
    "            print(\"dby:\", dby.shape)\n",
    "            print(\"dh:\", dh.shape)\n",
    "            \n",
    "            print(\"-------------\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    # clip to mitigate exploding gradients\n",
    "    if clipping:\n",
    "        for dparam in [dWex, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    gradients = (dWex, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby)\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(memory, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "      h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    h, c = memory\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "\n",
    "        # forward pass again, but we do not have to store the activations now\n",
    "        \n",
    "        # Embedding of input x\n",
    "        wes = Wex @ x\n",
    "        \n",
    "        # Concate hidden state and embedding\n",
    "        z = np.vstack((h, wes))\n",
    "        \n",
    "        # Forget gate\n",
    "        f =  sigmoid(Wf @ z + bf)\n",
    "        \n",
    "        # Input gate\n",
    "        i = sigmoid(Wi @ z + bi)\n",
    "        \n",
    "        # Candidate cell state\n",
    "        c_ = np.tanh(Wc @ z + bc)\n",
    "        \n",
    "        # New cell state\n",
    "        c = f * c + i * c_\n",
    "        \n",
    "        # Output gate\n",
    "        o = sigmoid(Wo @ z + bo)\n",
    "        \n",
    "        # New hidden state\n",
    "        h = o * np.tanh(c)\n",
    "        \n",
    "        # output layer (unnormalized)\n",
    "        y = Why @ h + by\n",
    "        \n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "\n",
    "        index = ix\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[index] = 1\n",
    "        ixes.append(index)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "dy: (65, 32)\n",
      "dWhy: (65, 256)\n",
      "dby: (65, 32)\n",
      "dh: (256, 32)\n",
      "-------------\n",
      "iter 0, loss: 534.443279\n",
      "0\n",
      "param: (256, 272)\n",
      "dparam: (256, 272)\n",
      "mem: (256, 272)\n",
      "1\n",
      "param: (256, 272)\n",
      "dparam: (256, 272)\n",
      "mem: (256, 272)\n",
      "2\n",
      "param: (256, 272)\n",
      "dparam: (256, 272)\n",
      "mem: (256, 272)\n",
      "3\n",
      "param: (256, 272)\n",
      "dparam: (256, 272)\n",
      "mem: (256, 272)\n",
      "4\n",
      "param: (256, 1)\n",
      "dparam: (256, 32)\n",
      "mem: (256, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (256,1) doesn't match the broadcast shape (256,32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-19b1b5b9d18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dparam:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mem:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mmem\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# adagrad update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (256,1) doesn't match the broadcast shape (256,32)"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "n_updates = 0\n",
    "\n",
    "# momentum variables for Adagrad\n",
    "mWex, mWhy = np.zeros_like(Wex), np.zeros_like(Why)\n",
    "mby = np.zeros_like(by)\n",
    "\n",
    "mWf, mWi, mWo, mWc = np.zeros_like(Wf), np.zeros_like(Wi), np.zeros_like(Wo), np.zeros_like(Wc)\n",
    "mbf, mbi, mbo, mbc = np.zeros_like(bf), np.zeros_like(bi), np.zeros_like(bo), np.zeros_like(bc)\n",
    "\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "data_length = cut_stream.shape[1]\n",
    "\n",
    "while n < 10:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p + seq_length + 1 >= data_length or n == 0:\n",
    "        hprev = np.zeros((hidden_size, batch_size))  # reset RNN memory\n",
    "        cprev = np.zeros((hidden_size, batch_size))\n",
    "        p = 0  # go from start of data\n",
    "\n",
    "    inputs = cut_stream[:, p : p + seq_length].T\n",
    "    targets = cut_stream[:, p + 1 : p + 1 + seq_length].T\n",
    "\n",
    "    # sample from the model now and then\n",
    "#     if n % 200 == 0:\n",
    "#         h_zero = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "#         c_zero = np.zeros((hidden_size, 1))\n",
    "#         sample_ix = sample((h_zero, c_zero), inputs[0][0], 2000)\n",
    "#         txt = \"\".join(ix_to_char[ix] for ix in sample_ix)\n",
    "#         print(\"----\\n %s \\n----\" % (txt,))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, activations, memory = forward(inputs, targets, (hprev, cprev))\n",
    "    hprev, cprev = memory\n",
    "    gradients = backward(activations)\n",
    "\n",
    "    dWex, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby = gradients\n",
    "    smooth_loss = smooth_loss * 0.999 + loss / batch_size * 0.001\n",
    "    if n % 20 == 0:\n",
    "        print(\"iter %d, loss: %f\" % (n, smooth_loss))  # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for index, (param, dparam, mem) in enumerate(zip(\n",
    "        [Wf, Wi, Wo, Wc, bf, bi, bo, bc, Wex, Why, by],\n",
    "        [dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWex, dWhy, dby],\n",
    "        [mWf, mWi, mWo, mWc, mbf, mbi, mbo, mbc, mWex, mWhy, mby],\n",
    "    )):\n",
    "        print(index)\n",
    "        print(\"param:\", param.shape)\n",
    "        print(\"dparam:\", dparam.shape)\n",
    "        print(\"mem:\", mem.shape)\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter\n",
    "    n_updates += 1\n",
    "    if n_updates >= max_updates:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-197902098d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdWex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c4f26e3f7c29>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(activations, clipping)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdcnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "data_length = cut_stream.shape[1]\n",
    "\n",
    "p = 0\n",
    "# inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
    "# targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
    "inputs = cut_stream[:, p : p + seq_length].T\n",
    "targets = cut_stream[:, p + 1 : p + 1 + seq_length].T\n",
    "\n",
    "delta = 0.0001\n",
    "\n",
    "hprev = np.zeros((hidden_size, batch_size))\n",
    "cprev = np.zeros((hidden_size, batch_size))\n",
    "\n",
    "memory = (hprev, cprev)\n",
    "\n",
    "loss, activations, hprev = forward(inputs, targets, memory)\n",
    "gradients = backward(activations, clipping=False)\n",
    "dWex, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby = gradients\n",
    "\n",
    "for weight, grad, name in zip(\n",
    "    [Wf, Wi, Wo, Wc, bf, bi, bo, bc, Wex, Why, by],\n",
    "        [dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWex, dWhy, dby],\n",
    "        [\"Wf\", \"Wi\", \"Wo\", \"Wc\", \"bf\", \"bi\", \"bo\", \"bc\", \"Wex\", \"Why\", \"by\"],\n",
    "    ):\n",
    "\n",
    "        str_ = \"Dimensions dont match between weight and gradient %s and %s.\" % (weight.shape, grad.shape)\n",
    "        assert weight.shape == grad.shape, str_\n",
    "\n",
    "        print(name)\n",
    "        countidx = 0\n",
    "        gradnumsum = 0\n",
    "        gradanasum = 0\n",
    "        relerrorsum = 0\n",
    "        erroridx = []\n",
    "\n",
    "        for i in range(weight.size):\n",
    "\n",
    "            # evaluate cost at [x + delta] and [x - delta]\n",
    "            w = weight.flat[i]\n",
    "            weight.flat[i] = w + delta\n",
    "            loss_positive, _, _ = forward(inputs, targets, memory)\n",
    "            weight.flat[i] = w - delta\n",
    "            loss_negative, _, _ = forward(inputs, targets, memory)\n",
    "            weight.flat[i] = w  # reset old value for this parameter\n",
    "            # fetch both numerical and analytic gradient\n",
    "            grad_analytic = grad.flat[i]\n",
    "            grad_numerical = (loss_positive - loss_negative) / (2 * delta)\n",
    "            gradnumsum += grad_numerical\n",
    "            gradanasum += grad_analytic\n",
    "            rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "            if rel_error is None:\n",
    "                rel_error = 0.0\n",
    "            relerrorsum += rel_error\n",
    "\n",
    "            if rel_error > 0.001:\n",
    "                print(\"WARNING %f, %f => %e \" % (grad_numerical, grad_analytic, rel_error))\n",
    "                countidx += 1\n",
    "                erroridx.append(i)\n",
    "\n",
    "        print(\n",
    "            \"For %s found %i bad gradients; with %i total parameters in the vector/matrix!\"\n",
    "            % (name, countidx, weight.size)\n",
    "        )\n",
    "        print(\n",
    "            \" Average numerical grad: %0.9f \\n Average analytical grad: %0.9f \\n Average relative grad: %0.9f\"\n",
    "            % (gradnumsum / float(weight.size), gradanasum / float(weight.size), relerrorsum / float(weight.size))\n",
    "        )\n",
    "        print(\" Indizes at which analytical gradient does not match numerical:\", erroridx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
